# MAGDA Python Implementation

This directory contains the Python implementation of MAGDA (Multi Agent Domain Automation), featuring a domain-agnostic architecture that can work with any domain (DAW, Desktop, Web, etc.).

## ğŸ“ Directory Structure

```
python/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ pyproject.toml              # Python package configuration
â”œâ”€â”€ uv.lock                     # Dependency lock file
â”œâ”€â”€ env.example                 # Environment variables template
â”œâ”€â”€ pytest.ini                 # Pytest configuration
â”œâ”€â”€ magda/                      # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                   # Domain-agnostic core
â”‚   â”‚   â”œâ”€â”€ domain.py          # Abstract interfaces
â”‚   â”‚   â””â”€â”€ pipeline.py        # Main pipeline
â”‚   â”œâ”€â”€ domains/                # Domain implementations
â”‚   â”‚   â””â”€â”€ daw/               # DAW domain (implemented)
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ daw_agents.py  # DAW-specific agents
â”‚   â”‚       â””â”€â”€ daw_factory.py # DAW factory
â”‚   â”œâ”€â”€ agents/                 # Legacy agents (to be migrated)
â”‚   â”œâ”€â”€ config.py              # Configuration
â”‚   â”œâ”€â”€ models.py              # Data models
â”‚   â””â”€â”€ utils.py               # Utilities
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ example_domain_agnostic.py
â”œâ”€â”€ benchmarks/                 # Performance benchmarks
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ run_model_benchmark.py
â”‚   â”œâ”€â”€ analyze_benchmark.py
â”‚   â””â”€â”€ *.json                 # Benchmark results
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ check_*.py             # Model checking scripts
â”‚   â””â”€â”€ debug_*.py             # Debugging scripts
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_*.py              # Unit and integration tests
â”‚   â””â”€â”€ conftest.py            # Pytest configuration
â””â”€â”€ docs/                       # Documentation (future)
    â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12+
- [uv](https://docs.astral.sh/uv/) package manager
- OpenAI API key

### Installation

1. **Install dependencies**:
   ```bash
   uv pip install -e '.[dev,docs]'
   ```

2. **Set up environment**:
   ```bash
   cp env.example .env
   # Edit .env and add your OpenAI API key
   ```

3. **Run tests**:
   ```bash
   pytest
   ```

## ğŸ—ï¸ Architecture

### Domain-Agnostic Core
- **`magda/core/domain.py`**: Abstract interfaces for domains, agents, and pipelines
- **`magda/core/pipeline.py`**: Main domain-agnostic pipeline

### Domain Implementations
- **`magda/domains/daw/`**: DAW domain implementation (currently implemented)
- **Future domains**: Desktop, Web, Mobile, Cloud, Business

### Key Components
- **DomainAgent**: Abstract base for all agents
- **DomainOrchestrator**: Abstract base for orchestrators
- **DomainPipeline**: Abstract base for pipelines
- **DomainFactory**: Factory pattern for domain creation

## ğŸ“š Examples

### Running Examples
```bash
# Domain-agnostic demo
python examples/example_domain_agnostic.py
```

### Example Usage
```python
from magda.core.domain import DomainType
from magda.core.pipeline import MAGDACorePipeline
from magda.domains.daw import DAWFactory

# Create DAW pipeline
daw_factory = DAWFactory()
pipeline = MAGDACorePipeline(daw_factory, DomainType.DAW)

# Set host context
pipeline.set_host_context({
    "vst_plugins": ["serum", "addictive drums"],
    "track_names": ["bass", "drums", "guitar"]
})

# Process prompts
result = pipeline.process_prompt("create bass track with serum")
```

## ğŸ§ª Testing

### Running Tests
```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_*.py
pytest benchmarks/test_*.py

# Run with coverage
pytest --cov=magda --cov-report=html
```

### Test Categories
- **Unit Tests**: `tests/test_*.py`
- **Integration Tests**: `tests/test_integration.py`
- **Benchmark Tests**: `benchmarks/test_*.py`

## ğŸ“Š Benchmarks

### Running Benchmarks
```bash
# Model performance benchmark
python benchmarks/run_model_benchmark.py

# Operations benchmark
python benchmarks/test_operations_benchmark.py

# Analyze results
python benchmarks/analyze_benchmark.py benchmarks/results.json
```

### Benchmark Categories
- **Model Performance**: Tests different OpenAI models
- **Complexity Detection**: Compares algorithmic vs semantic detection
- **Operations**: Tests actual DAW operations

## ğŸ› ï¸ Development

### Adding New Domains
1. Create domain directory: `magda/domains/your_domain/`
2. Implement agents inheriting from `DomainAgent`
3. Create factory implementing `DomainFactory`
4. Add tests and documentation

### Code Quality
```bash
# Linting
ruff check .

# Type checking
mypy magda/

# Security checks
bandit -r magda/
```

### Documentation
```bash
# Install docs dependencies
uv pip install '.[docs]'

# Build documentation (when mkdocs is set up)
mkdocs build
```

## ğŸ”§ Scripts

### Utility Scripts
```bash
# Check available models
python scripts/check_available_models.py

# Debug specific functionality
python scripts/debug_clip.py

# Capture sample data
python scripts/capture_samples.py
```

## ğŸ“¦ Package Management

### Dependencies
- **Core**: `openai`, `pydantic`, `python-dotenv`
- **Dev**: `pytest`, `ruff`, `mypy`, `bandit`
- **Docs**: `mkdocs`, `mkdocs-material`, `mkdocstrings`

### Installation Commands
```bash
# Core package
uv pip install -e .

# With development dependencies
uv pip install -e '.[dev]'

# With documentation dependencies
uv pip install -e '.[docs]'

# With all dependencies
uv pip install -e '.[dev,docs]'
```

## ğŸ¤ Contributing

### Development Workflow
1. **Fork the repository**
2. **Create a feature branch**
3. **Make your changes**
4. **Add tests**
5. **Update documentation**
6. **Submit a pull request**

### Code Standards
- Use type hints throughout
- Follow Google-style docstrings
- Include examples in docstrings
- Add tests for new features
- Update relevant documentation

### Testing Guidelines
- Write unit tests for all new code
- Include integration tests for complex features
- Add benchmark tests for performance-critical code
- Ensure all tests pass before submitting PR

## ğŸ“„ License

This project is licensed under the GPL-3.0 License - see the [LICENSE](../LICENSE) file for details.

---

For more information, see:
- [Main README](../README.md) - Project overview
- [Examples](examples/README.md) - Usage examples
- [Benchmarks](benchmarks/README.md) - Performance testing
- [Scripts](scripts/README.md) - Utility scripts
